---
title: "[Paper Review] AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"
date: "2026-01-23 18:00:00 +0900"
categories: [Paper Review]
tags: [Paper Review, 3D Vision]
math: true
---

Author: [Alexey Dosovitskiy](https://arxiv.org/search/cs?searchtype=author&query=Dosovitskiy,+A), [Lucas Beyer](https://arxiv.org/search/cs?searchtype=author&query=Beyer,+L), [Alexander Kolesnikov](https://arxiv.org/search/cs?searchtype=author&query=Kolesnikov,+A), [Dirk Weissenborn](https://arxiv.org/search/cs?searchtype=author&query=Weissenborn,+D), [Xiaohua Zhai](https://arxiv.org/search/cs?searchtype=author&query=Zhai,+X), [Thomas Unterthiner](https://arxiv.org/search/cs?searchtype=author&query=Unterthiner,+T), [Mostafa Dehghani](https://arxiv.org/search/cs?searchtype=author&query=Dehghani,+M), [Matthias Minderer](https://arxiv.org/search/cs?searchtype=author&query=Minderer,+M), [Georg Heigold](https://arxiv.org/search/cs?searchtype=author&query=Heigold,+G), [Sylvain Gelly](https://arxiv.org/search/cs?searchtype=author&query=Gelly,+S), [Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&query=Uszkoreit,+J), [Neil Houlsby](https://arxiv.org/search/cs?searchtype=author&query=Houlsby,+N)

[[paper]](https://arxiv.org/abs/2010.11929) [[github]](https://github.com/google-research/vision_transformer)



### 1 INTRODUCTION

---

다양한 컴퓨터 비전 연구에서 트랜스포머를 도입하려고 했지만 독특한 attention 패턴 때문에 아직까지 ResNet과 같은 구조가 여전히 SOTA로 남아 있습니다. 

ViT는 기존 트랜스포머를 최소한으로 수정해서 이미지에 바로 적용합니다. 이를 위해서 ViT는 이미지를 여러 패치로 나누고 각 패치에 sequence of linear embeddings을 트랜스포머로 제공합니다. 각 이미지 패치들은 토큰(단어)으로 동일하게 취급됩니다. 그리고 image classification의 지도학습 방식으로 훈련합니다. 

보통 크기의 데이터셋(ImageNet)에서 강력한 정규화 없이 훈련했을 때, ResNet과 비슷한 사이즈의 ViT는 조금 낮은 정확도를 보입니다. ViT는 CNN에 내재된 translation equivariance와 locality와 같은 일부 귀납적 편향이 부족하므로 불충분한 크기의 데이터셋으로 학습될 때 일반화 성능이 좋지 않습니다.

그러나 만약 더 큰 데이터셋(14M-300M images)로 훈련된다면 상황이 달라집니다. ViT가 충분한 규모의 데이터셋으로 학습되고 적은 datapoints의 task로 전이될 때 좋은 결과를 도출합니다. ViT가 public ImageNet-21k나 자체 JFT-300M 데이터셋으로 학습되었을 때 다중 이미지 인식에서 SOTA를 달성합니다. 가장 좋은 모델에서는 ImageNet에서 88.55%, ImageNet-ReaL에서 90.72%, CIFAR-100에서 94.55%, 19개의 task로 구성된 VTAB에서 77.63%를 달성합니다.

### 3 METHOD

---

![Figure 1](assets/img/Paper-Review/ViT/Figure1.webp)

ViT는 트랜스포머의 구조를 가능한 최대한 따름으로써 확장 가능하고 효율적인 구현을 별도의 작업 없이 바로 사용할 수 있게 하였습니다.

#### 3.1 VISION TRANSFORMER (VIT)

트랜스포머는 입력으로 1D squence of token embeddings을 받습니다. 2D인 이미지를 다루기 위해 ViT는 이미지$$x \in \mathbb{R}^{H \times W \times C}$$를  sequence of flattened 2D 패치 $$x_p \in \mathbb{R}^{N \times (P^{2}\cdot C)}$$으로 바꿉니다. 여기서 $$(H, W)$$는 원본 이미지의 해상도, $$C$$는 채널의 수, $$(P, P)$$는 각 이미지 패치의 해상도, $$N=HW/P^{2}$$은 패치의 수이면서 트랜스포머의 입력 sequence의 길이 역할도 합니다. 트랜스포머는 모든 layer에서 일정한 latent vector size $$D$$를 사용합니다. 그래서 ViT는 각 패치들에 flatten을 수행하며 학습 가능한 선형 투영과 함께 $$D$$차원으로 맵핑합니다. ViT는 투영의 결과를 패치 임베딩이라고 부릅니다.


$$
z_0 = [x_{\mathrm{class}};\, x_p^{1}E;\, x_p^{2}E;\, \cdots;\, x_p^{N}E] + E_{\mathrm{pos}},
\quad
E \in \mathbb{R}^{(P^{2}\cdot C)\times D},
\quad
E_{\mathrm{pos}} \in \mathbb{R}^{(N+1)\times D}
$$


ViT는 임베딩된 패치$$(z_0^{0} = x_{\mathrm{class}})$$의 시퀀스에 학습 가능한 임베딩을 앞에 붙입니다. 임베딩의 상태는 트랜스포머 인코더$$r\left(z_L^{0}\right)$$에서 이미지 표현 $$y$$역할을 합니다


$$
y = \mathrm{LN}\left(z_L^{0}\right)
$$


pre-training과 fine-tuning 둘 다 classification head가 $$(z_L^{0} = x_{\mathrm{class}})$$에 붙습니다. classification head는 pre-training에서는 하나의 hidden layer를 가진 MLP로, fine-tuning에서 single linear layer로 구현됩니다.

Position embedding은 위치 정보를 보존하기위해 패치 임베딩에 더해집니다. ViT는 더 향상된 2D-aware 위치 임베딩을 사용한다고해서 더 좋은 성능을 가지는 것을 관찰하지 못했기 때문에 표준 학습 가능한 1D 위치 임베딩을 사용합니다. 최종적인 sequence of embedding vector는 인코더의 입력으로 사용됩니다.

트랜스포머 인코더는 multi-head self-attention과 MLP block가 교대로 layer를 이룹니다. 


$$
z'_{\ell} = \mathrm{MSA}\big(\mathrm{LN}(z_{\ell-1})\big) + z_{\ell-1},
\quad \ell = 1,\ldots,L
$$

$$
z_{\ell} = \mathrm{MLP}\big(\mathrm{LN}(z'_{\ell})\big) + z'_{\ell},
\quad \ell = 1,\ldots,L
$$



LayerNorm은 모든 block전에 적용되며 residual connection은 모든 block 이후에 적용됩니다.  MLP는 GELU 비선형성을 가진 두 layers로 구성됩니다.

**Inductive bias:** ViT는 CNN보다 매우 적은 이미지 특화 inductive biase를 가집니다. CNN에서 locality, 2D neighborhood structure, 그리고 translation equivariance는 모든 layer에 내재되어 있습니다. ViT에서는 오직 MlP layer에서만 local과 translationally equivariant를 가지며 self-attention layer는 전역적입니다. 2D neighborhood structure는 매우 드물게 사용됩니다. 모델의 시작 부분에서 이미지를 패치로 자르는 부분과 다른 해상도인 이미지에 대한 위치 임베딩을  조정하기 위해 fine-tuning 시에만 사용됩니다. 그 외에는 초기에 위치 임베딩은 패치의 2D 위치에 대한 정보는 가져오지 않으며 패치 간 모든 공간적 관계는 처음부터 학습되어야 합니다.

**Hybrid Architecture:** 원본 이미지 패치 대신에 입력 sequence는 CNN의 feature map으로 형성될 수 있습니다. 이 하이브리드 모델은 패치 임베딩 투영이 CNN feature map에서 추출된 패치에 적용됩니다. 특별한 경우에 패치는 부분적 1x1 크기를 가질 수 있으며  이는 입력 sequence가 feature map의 공간 차원을 단순하게 flatten을 수행하고 트랜스포머 차원으로 투영하여 얻을 수 있음을 의미합니다. 그 classification 입력 임베딩과 위치 임베딩은 더해집니다.

#### 3.2 FINE-TUNING AND HIGHER RESOLUTION

전형적으로 ViT를 large dataset으로 사전 학습하고 더 작은 downstream task에 fine-tuning을 진행합니다. 이를 위해 ViT는 사전 학습된 prediction head는 삭제하고 zero-initialized $$D × K$$ feedforward layer를 붙입니다. 여기서 K는 downstream class의 수입니다. 더 높은 해상도에서 fine-tuning을 수행하는 것이 사전 학습을 하는것 보다 종종 유용합니다. 더 높은 해상도의 이미지 입력을 받을 때 같은 패치 크기를 유지하므로 결과적으로 더 큰 sequence 길이가 됩니다. ViT는 메모리가 버티는 한 임의의 sequence 길이를 처리할 수 있습니다. 그러나 사전 학습된 위치 임베딩은 더 이상 의미가 없습니다. 그러므로 ViT는 각 패치의 본래 이미지에서의 위치에 따라 사전 학습된 위치 임베딩의 2D interpolation을 수행합니다. 이 해상도 조정과 패치 추출은 ViT에서 이미지의 2D 구조에 대한 inductive bias가 수동적으로 주입되는 유일한 지점입니다.

### 4 EXPERIMENTS

---

![Table 1](assets/img/Paper-Review/ViT/Table1.webp)

BERT에 사용된 설정을 기반으로 ViT 구성을 만들었으며 "Base"와 "Large"모델은 BERT에서 채택하였고 "Huge"모델을 추가했습니다. 모델 크기와 입력 패치 크기를 나타내기 위해 ViT-L/16과 같이 간략한 표기법을 사용합니다. 트랜스포머의 sequence 길이는 패치 크기의 제곱에 반비례하므로 더 작은 패치 크기를 가진 모델이 computational cost가 많이 듭니다.

기본 CNN의 경우 ResNet을 사용하지만 Batch Normalization layer를 Group Normalization으로 교체하고 standardized convolution을 사용했습니다. 이러한 수정은 전이 성능을 향상 시키며 수정된 모델을 ResNet(BiT)로 표기합니다. 하이브리드 모델의 경우 중간 feature map을 패치 크기가 하나인 픽셀인 ViT에 입력합니다. 다른 시퀀스 길이를 실험하기 위해 (i) 일반 ResNet50의 스테이지4 출력을 사용하거나 (ii) 스테이지4를 제거하고 스테이지3에 동일한 수의 레이어를 배치한 후 확장된 스테이지 3의 출력을 사용합니다. 옵션(ii)는 4배 더 긴 시퀀스 길이와 더 높은 cost의 ViT 모델을 초래합니다.

**Fine-tuning:** $$B_1=0.9$$, $$B_2=0.999$$인 Adam을 사용하였으며 배치 크기 4096으로 모든 모델을 학습시키며, 0.1의 높은 weight decay를 적용합니다. 이는 모든 모델의 전이에 유용하다는 것을 발견했습니다. ViT에서 Adam이 ResNet에 대해 SGD보다 약간 더 잘 작동함을 보여줍니다. linear warmup과 decay를 사용합니다. fine-tuning을 위해 모든 모델에 대해 모멘텀이 있는 SGD와 배치 크기 512를 사용합니다. Table 2의 ImageNet 결과는 ViT-L/16은 512, ViT-H/14는 518의 더 높은 해상도로 fine-tuning을 수행하였으며 0.9999의 계수를 가진 averaging도 사용했습니다.

![Table 2](assets/img/Paper-Review/ViT/Table2.webp)

![Figure 2](assets/img/Paper-Review/ViT/Figure2.webp)

![Figure 3](assets/img/Paper-Review/ViT/Figure3.webp)

![Figure 4](assets/img/Paper-Review/ViT/Figure4.webp)

![Figure 5](assets/img/Paper-Review/ViT/Figure5.webp)

![Figure 6](assets/img/Paper-Review/ViT/Figure6.webp)

![Figure 7](assets/img/Paper-Review/ViT/Figure7.webp)

### 5 CONCLUSION

---

ViT는 많은 이미지 분류 데이터셋에서 SOTA와 동등하거나 그 이상을 달성하면서 사전 학습 비용도 상대적으로 저렴합니다.


